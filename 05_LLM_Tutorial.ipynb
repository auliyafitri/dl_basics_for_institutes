{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbgcy_Lcnb6K"
   },
   "source": [
    "# Building a Tiny Decoder-Only Transformer\n",
    "\n",
    "Welcome! We'll re-create a GPT-style decoder with nothing more than PyTorch. The walkthrough is broken into:\n",
    "\n",
    "- Core building blocks (normalization, attention, feed-forward)\n",
    "- Assembling the full language model\n",
    "- Training on Tiny Shakespeare and generating text\n",
    "- Experimenting with sampling tricks (top-k, top-p, temperature)\n",
    "\n",
    "Each section intentionally slows down to explain both the math and the PyTorch mechanics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab. Installing necessary packages...\")\n",
    "    !git clone https://github.com/auliyafitri/dl_basics_for_institutes.git\n",
    "    %cd dl_basics_for_institutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfvB3W9lDGJN"
   },
   "outputs": [],
   "source": [
    "# Pin the datasets package version to avoid transient download errors on Colab runtimes.\n",
    "!pip install datasets==3.6.0\n",
    "!pip install seaborn\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrQMXHWj4nKj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def set_seed(value: int = 42) -> None:\n",
    "    \"\"\"Keep experiments deterministic enough for demo purposes.\"\"\"\n",
    "    torch.manual_seed(value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(value)\n",
    "\n",
    "\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ2VP8xCpMYw"
   },
   "source": [
    "## Model Blueprint\n",
    "\n",
    "We'll keep every hyperparameter in a single dataclass so it's easy to tweak knobs later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAIQppvnMOeZ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelSettings:\n",
    "    embedding_dim: int = 512\n",
    "    feedforward_dim: int = 4 * 512\n",
    "    blocks: int = 2\n",
    "    dropout: float = 0.1\n",
    "    vocab_size: int = 10_000\n",
    "    context_length: int = 128\n",
    "\n",
    "\n",
    "cfg = ModelSettings()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6gyx0Q9Llde"
   },
   "source": [
    "### Dissecting a Transformer Block\n",
    "\n",
    "Each decoder block follows a *norm → attention → residual → norm → MLP → residual* pattern. We'll rebuild every piece so the math never feels like a black box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xc5qVYV3Nz-I",
    "outputId": "d39a26a7-2b8b-457c-bde6-9c7dcd7d65cd"
   },
   "outputs": [],
   "source": [
    "class CustomLayerNorm(nn.Module):\n",
    "    \"\"\"Manual implementation of layer normalization for educational clarity.\"\"\"\n",
    "\n",
    "    def __init__(self, width: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(width))\n",
    "        self.bias = nn.Parameter(torch.zeros(width))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # tensor: (batch, sequence, feature)\n",
    "        mean = tensor.mean(dim=-1, keepdim=True)\n",
    "        variance = tensor.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized = (tensor - mean) / torch.sqrt(variance + self.eps)\n",
    "        return normalized * self.weight + self.bias\n",
    "\n",
    "\n",
    "demo_norm = CustomLayerNorm(cfg.embedding_dim)\n",
    "dummy = torch.randn(1, 3, cfg.embedding_dim)\n",
    "print(\"LayerNorm demo output shape:\", demo_norm(dummy).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6LJFxvYtD7R"
   },
   "outputs": [],
   "source": [
    "class SelfAttentionModule(nn.Module):\n",
    "    \"\"\"Single-head self-attention with a triangular mask for autoregressive decoding.\"\"\"\n",
    "\n",
    "    def __init__(self, width: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.qkv_proj = nn.Linear(width, 3 * width)\n",
    "        self.out_proj = nn.Linear(width, width)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.out_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, return_weights: bool = False):\n",
    "        batch, seq_len, features = tokens.shape\n",
    "        if features != self.width:\n",
    "            raise ValueError(\n",
    "                f\"Expected embedding size {self.width}, but received {features}.\"\n",
    "            )\n",
    "\n",
    "        q, k, v = self.qkv_proj(tokens).chunk(3, dim=-1)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * (self.width**-0.5)\n",
    "\n",
    "        mask = torch.ones(seq_len, seq_len, device=tokens.device).tril()\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        weights = self.attn_dropout(weights)\n",
    "\n",
    "        attended = torch.matmul(weights, v)\n",
    "        projected = self.out_proj(attended)\n",
    "        projected = self.out_dropout(projected)\n",
    "\n",
    "        if return_weights:\n",
    "            return projected, weights\n",
    "        return projected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrqrjHl7ywsX"
   },
   "source": [
    "Let's peek at the attention map on random data to confirm the masking behaves as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "oaiHyeoqyraZ",
    "outputId": "ffb2cc4a-bbf0-4a12-843c-66f922f33a65"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "attention = SelfAttentionModule(cfg.embedding_dim, dropout=0.0)\n",
    "sample_tokens = torch.randn(1, 6, cfg.embedding_dim)\n",
    "_, attn_weights = attention(sample_tokens, return_weights=True)\n",
    "weights_np = attn_weights.squeeze(0).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "sns.heatmap(weights_np, cmap=\"magma\", cbar=True, square=True)\n",
    "plt.title(\"Attention mask in action\")\n",
    "plt.xlabel(\"Key index\")\n",
    "plt.ylabel(\"Query index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psUlysBkKo94",
    "outputId": "34426694-9208-4ce4-8d2c-7cde23feec9e"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Two-layer MLP used inside each transformer block.\"\"\"\n",
    "\n",
    "    def __init__(self, width: int, hidden: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(width, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, width),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8yBkjCLNMlj"
   },
   "source": [
    "With normalization, attention, and the feed-forward network ready, we can wire up the full decoder block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tGDbfH6NQID",
    "outputId": "fc7be2c1-d945-4311-dd98-825d33061680"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, width: int, ff_hidden: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_attn = CustomLayerNorm(width)\n",
    "        self.attention = SelfAttentionModule(width, dropout)\n",
    "        self.norm_ff = CustomLayerNorm(width)\n",
    "        self.feedforward = FeedForward(width, ff_hidden, dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        normed = self.norm_attn(x)\n",
    "        attn_out = self.attention(normed)\n",
    "        x = normed + attn_out\n",
    "\n",
    "        normed_ff = self.norm_ff(x)\n",
    "        ff_out = self.feedforward(normed_ff)\n",
    "        x = normed_ff + ff_out\n",
    "        return x\n",
    "\n",
    "\n",
    "test_block = TransformerBlock(cfg.embedding_dim, cfg.feedforward_dim)\n",
    "print(\n",
    "    \"Transformer block output shape:\",\n",
    "    test_block(torch.randn(1, 3, cfg.embedding_dim)).shape,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIT2gSUtf3M_"
   },
   "source": [
    "### Assembling the Language Model\n",
    "\n",
    "A minimal GPT-style model stacks several decoder blocks, adds token & position embeddings, and projects back to vocabulary logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHXI6NE32YBv",
    "outputId": "7a3c1413-2251-4a03-bb30-c87f09cbb0ad"
   },
   "outputs": [],
   "source": [
    "class MiniDecoderLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        ff_hidden_dim: int,\n",
    "        context_length: int,\n",
    "        layers: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embeddings = nn.Embedding(context_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embedding_dim, ff_hidden_dim, dropout)\n",
    "                for _ in range(layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        batch, seq_len = token_ids.shape\n",
    "        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        hidden = self.token_embeddings(token_ids) + self.position_embeddings(positions)\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            hidden = block(hidden)\n",
    "\n",
    "        hidden = self.norm(hidden)\n",
    "        logits = self.lm_head(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "demo_ids = torch.randint(0, cfg.vocab_size, (1, 4))\n",
    "toy_model = MiniDecoderLM(\n",
    "    vocab_size=cfg.vocab_size,\n",
    "    embedding_dim=cfg.embedding_dim,\n",
    "    ff_hidden_dim=cfg.feedforward_dim,\n",
    "    context_length=cfg.context_length,\n",
    "    layers=cfg.blocks,\n",
    "    dropout=cfg.dropout,\n",
    ")\n",
    "print(toy_model)\n",
    "print(\"Logit tensor shape:\", toy_model(demo_ids).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM3gL5p6PRfy"
   },
   "source": [
    "## Training and Text Generation\n",
    "\n",
    "Now that the architecture stands, let's prepare data, train the model, and see it generate text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyRSdqc2vCw0"
   },
   "source": [
    "### Preparing the Dataset\n",
    "\n",
    "We'll lean on the Tiny Shakespeare corpus because it's small, public, and perfect for quick experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V-LNpgjDBOL"
   },
   "outputs": [],
   "source": [
    "# Load raw text splits\n",
    "train_raw = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\")\n",
    "valid_raw = load_dataset(\"karpathy/tiny_shakespeare\", split=\"validation\")\n",
    "\n",
    "# GPT-2 tokenizer keeps things simple and gives us byte-level BPEs\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update configuration now that we know the vocabulary size\n",
    "cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"])\n",
    "\n",
    "\n",
    "train_tokenized = train_raw.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "valid_tokenized = valid_raw.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "train_stream = list(chain.from_iterable(train_tokenized[\"input_ids\"]))\n",
    "valid_stream = list(chain.from_iterable(valid_tokenized[\"input_ids\"]))[:2000]\n",
    "\n",
    "\n",
    "class AutoregressiveDataset(Dataset):\n",
    "    def __init__(self, stream, block_size):\n",
    "        self.stream = stream\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stream) - self.block_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        window = self.stream[index : index + self.block_size + 1]\n",
    "        x = torch.tensor(window[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(window[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "sequence_length = cfg.context_length\n",
    "train_dataset = AutoregressiveDataset(train_stream, sequence_length)\n",
    "valid_dataset = AutoregressiveDataset(valid_stream, sequence_length)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, drop_last=True, pin_memory=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=16, shuffle=False, drop_last=True, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Batches in training loader: {len(train_loader)}\")\n",
    "print(f\"Batches in validation loader: {len(valid_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nev3XgQ6GqYD"
   },
   "source": [
    "We'll also decide where checkpoints should live so we can resume or perform inference later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgLF1mD9qchS",
    "outputId": "9310cf50-5491-4e8a-fe15-230cad39761a"
   },
   "outputs": [],
   "source": [
    "# Choose where to persist checkpoints.\n",
    "\n",
    "\n",
    "# To keep everything inside the temporary runtime instead, uncomment:\n",
    "model_checkpoint_path = \"pretrained/mini_llm.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0U96GZvc3Qx"
   },
   "source": [
    "We're ready to launch the training loop. Feel free to interrupt the cell once you've seen enough iterations—the best checkpoint will already be written to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "V8NkQrt5Uzqj",
    "outputId": "6a09b350-08e3-4076-8d9c-794be692b1ef"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "def plot_history(train_steps, train_losses, val_steps, val_losses):\n",
    "    clear_output(wait=True)\n",
    "    plt.clf()\n",
    "    plt.plot(train_steps, train_losses, label=\"Train loss\", marker=\"o\")\n",
    "    plt.plot(val_steps, val_losses, label=\"Validation loss\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training progress\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    display(plt.gcf())\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    *,\n",
    "    device,\n",
    "    log_every=100,\n",
    "    eval_every=500,\n",
    "    patience=5,\n",
    "):\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    loss_buffer = []\n",
    "    train_steps, train_losses = [], []\n",
    "    val_steps, val_losses = [], []\n",
    "    checks_without_improvement = 0\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_buffer.append(loss.item())\n",
    "            global_step += 1\n",
    "            refresh_plot = False\n",
    "\n",
    "            if global_step % log_every == 0:\n",
    "                train_steps.append(global_step)\n",
    "                train_losses.append(sum(loss_buffer) / len(loss_buffer))\n",
    "                loss_buffer.clear()\n",
    "                refresh_plot = True\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                val_total = 0.0\n",
    "                batches = 0\n",
    "                with torch.no_grad():\n",
    "                    for v_inputs, v_targets in val_loader:\n",
    "                        v_inputs = v_inputs.to(device, non_blocking=True)\n",
    "                        v_targets = v_targets.to(device, non_blocking=True)\n",
    "                        v_logits = model(v_inputs)\n",
    "                        v_loss = criterion(\n",
    "                            v_logits.view(-1, v_logits.size(-1)), v_targets.reshape(-1)\n",
    "                        )\n",
    "                        val_total += v_loss.item()\n",
    "                        batches += 1\n",
    "                val_loss = val_total / max(1, batches)\n",
    "\n",
    "                if not val_losses or val_loss < min(val_losses):\n",
    "                    torch.save(model.state_dict(), model_checkpoint_path)\n",
    "                    checks_without_improvement = 0\n",
    "                else:\n",
    "                    checks_without_improvement += 1\n",
    "                    if checks_without_improvement > patience:\n",
    "                        print(\n",
    "                            f\"Early stopping triggered after {patience} unimproved checks.\"\n",
    "                        )\n",
    "                        plot_history(train_steps, train_losses, val_steps, val_losses)\n",
    "                        return\n",
    "\n",
    "                val_steps.append(global_step)\n",
    "                val_losses.append(val_loss)\n",
    "                model.train()\n",
    "                refresh_plot = True\n",
    "\n",
    "            if refresh_plot:\n",
    "                plot_history(train_steps, train_losses, val_steps, val_losses)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "model = MiniDecoderLM(\n",
    "    vocab_size=cfg.vocab_size,\n",
    "    embedding_dim=cfg.embedding_dim,\n",
    "    ff_hidden_dim=cfg.feedforward_dim,\n",
    "    context_length=cfg.context_length,\n",
    "    layers=cfg.blocks,\n",
    "    dropout=cfg.dropout,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "run_training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=1,\n",
    "    device=device,\n",
    "    log_every=100,\n",
    "    eval_every=500,\n",
    "    patience=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdpJYH_TZsdJ"
   },
   "source": [
    "### Using the Trained Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uqr-jBVNGkd"
   },
   "source": [
    "Load the checkpoint and rebuild the model with the exact same configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgPQ0oe0NBlM",
    "outputId": "bb54f9c5-33ed-49a2-e6d2-a983c83df45a"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MiniDecoderLM(\n",
    "    vocab_size=cfg.vocab_size,\n",
    "    embedding_dim=cfg.embedding_dim,\n",
    "    ff_hidden_dim=cfg.feedforward_dim,\n",
    "    context_length=cfg.context_length,\n",
    "    layers=cfg.blocks,\n",
    "    dropout=cfg.dropout,\n",
    ").to(device)\n",
    "\n",
    "state_dict = torch.load(model_checkpoint_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtDWu2nwNATm"
   },
   "source": [
    "#### Greedy Decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gkk_gYlHfSu5",
    "outputId": "87f835bb-c94a-4c25-a898-10c6ef0a64d1"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, input_ids, max_len, *, device=torch.device(\"cpu\")):\n",
    "    model.eval()\n",
    "    generated = input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_len - generated.size(1)):\n",
    "        logits = model(generated)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "prompt_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long)\n",
    "prompt_ids = prompt_ids.to(device)\n",
    "\n",
    "greedy_ids = greedy_decode(model, prompt_ids, max_len=128, device=device)\n",
    "text = tokenizer.decode(greedy_ids[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "print(\"==== Greedy decode ====\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qfjYCMA9cAr"
   },
   "source": [
    "#### Sampling\n",
    "\n",
    "Sampling adds controlled randomness to generation so outputs don't collapse to the same completion every time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0eRpOcXKpVJ"
   },
   "outputs": [],
   "source": [
    "def apply_top_k(logits, top_k):\n",
    "    if not isinstance(top_k, int) or top_k <= 0:\n",
    "        raise ValueError(f\"top_k must be a positive integer, received {top_k}\")\n",
    "    threshold = torch.topk(logits, top_k, dim=-1).values[..., -1, None]\n",
    "    filtered = torch.where(\n",
    "        logits < threshold, torch.full_like(logits, float(\"-inf\")), logits\n",
    "    )\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def apply_top_p(logits, top_p):\n",
    "    if not (0.0 < top_p <= 1.0):\n",
    "        raise ValueError(f\"top_p must lie in (0, 1], received {top_p}\")\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "    sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "    cumulative = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    mask = cumulative > top_p\n",
    "    mask[..., 0] = False\n",
    "    sorted_logits = sorted_logits.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "    restored = torch.full_like(logits, float(\"-inf\"))\n",
    "    restored.scatter_(-1, sorted_indices, sorted_logits)\n",
    "    return restored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MS5iJS4GZT3",
    "outputId": "f51b2829-9936-49ea-809c-fa2a23145484"
   },
   "outputs": [],
   "source": [
    "def sample_text(\n",
    "    model,\n",
    "    input_ids,\n",
    "    max_len,\n",
    "    *,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    temperature=1.0,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    model.eval()\n",
    "    generated = input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_len - generated.size(1)):\n",
    "        logits = model(generated)[:, -1, :]\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            logits = apply_top_k(logits, top_k)\n",
    "        if top_p is not None:\n",
    "            logits = apply_top_p(logits, top_p)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "prompt_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "sampled_ids = sample_text(\n",
    "    model,\n",
    "    prompt_ids,\n",
    "    max_len=128,\n",
    "    top_k=3,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"==== Sampled decode ====\")\n",
    "print(tokenizer.decode(sampled_ids[-1].tolist(), skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYzZBlnbyntI"
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Hugging Face GPT-2 implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) for a production-grade reference.\n",
    "- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) for a visual introduction to the architecture.\n",
    "- [llm.c](https://github.com/karpathy/llm.c) for a minimalist C implementation that mirrors these concepts.\n",
    "- Vaswani et al., \"Attention Is All You Need\" — the original Transformer paper.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
